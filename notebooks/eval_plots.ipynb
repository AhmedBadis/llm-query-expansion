{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Plots and Analysis\n",
        "\n",
        "This notebook provides visualization and analysis of retrieval evaluation results.\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join('..', 'src'))\n",
        "\n",
        "from eval.metrics import (\n",
        "    load_run_file,\n",
        "    load_qrels_file,\n",
        "    ndcg_at_k,\n",
        "    map_at_k,\n",
        "    recall_at_k,\n",
        "    mrr\n",
        ")\n",
        "from eval.stats_tests import compare_runs\n",
        "from eval.robustness_slices import load_slices\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load baseline run and qrels\n",
        "# Replace with actual paths when available\n",
        "baseline_run_path = '../results/baseline_runs/bm25_trec_covid.csv'\n",
        "qrels_path = '../dataset/trec-covid/qrels'\n",
        "\n",
        "# For now, use sample data\n",
        "baseline_run_path = '../tests/data/sample_run.csv'\n",
        "qrels_path = '../tests/data/sample_qrels.csv'\n",
        "\n",
        "baseline_run = load_run_file(baseline_run_path)\n",
        "qrels = load_qrels_file(qrels_path)\n",
        "\n",
        "print(f\"Loaded {len(baseline_run)} queries\")\n",
        "print(f\"Loaded {len(qrels)} qrels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute baseline metrics\n",
        "baseline_ndcg10 = ndcg_at_k(baseline_run, qrels, k=10)\n",
        "baseline_map = map_at_k(baseline_run, qrels)\n",
        "baseline_recall100 = recall_at_k(baseline_run, qrels, k=100)\n",
        "baseline_mrr = mrr(baseline_run, qrels)\n",
        "\n",
        "print(\"Baseline Metrics:\")\n",
        "print(f\"  nDCG@10:  {baseline_ndcg10:.4f}\")\n",
        "print(f\"  MAP:      {baseline_map:.4f}\")\n",
        "print(f\"  Recall@100: {baseline_recall100:.4f}\")\n",
        "print(f\"  MRR:      {baseline_mrr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Metrics Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Compare baseline vs system (when system run is available)\n",
        "# For demonstration, create placeholder data\n",
        "systems = ['Baseline (BM25)']\n",
        "ndcg_scores = [baseline_ndcg10]\n",
        "\n",
        "# Plot nDCG@10 comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(systems, ndcg_scores, color='steelblue', alpha=0.7)\n",
        "plt.ylabel('nDCG@10', fontsize=12)\n",
        "plt.title('nDCG@10 Comparison Across Systems', fontsize=14, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-Query Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute per-query nDCG@10\n",
        "from eval.stats_tests import compute_per_query_metric\n",
        "\n",
        "per_query_ndcg = compute_per_query_metric(baseline_run, qrels, metric='ndcg@10', k=10)\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(list(per_query_ndcg.values()), bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('nDCG@10', fontsize=12)\n",
        "plt.ylabel('Number of Queries', fontsize=12)\n",
        "plt.title('Distribution of nDCG@10 per Query', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Robustness Analysis: Familiar vs Unfamiliar Queries\n",
        "\n",
        "(Will be populated once query slices are computed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load query slices (if available)\n",
        "# slices_path = '../results/slices.csv'\n",
        "# slices = load_slices(slices_path)\n",
        "# \n",
        "# Compute metrics per slice and plot comparison\n",
        "print(\"Robustness analysis will be available once query slices are computed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Tests\n",
        "\n",
        "(Will be populated when comparing baseline vs system runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare baseline vs system (when available)\n",
        "# results = compare_runs(\n",
        "#     baseline_run_path,\n",
        "#     system_run_path,\n",
        "#     qrels_path,\n",
        "#     metric='ndcg@10',\n",
        "#     k=10\n",
        "# )\n",
        "# \n",
        "# print(\"Statistical Comparison:\")\n",
        "# print(f\"  Baseline Mean: {results['baseline_mean']:.4f}\")\n",
        "# print(f\"  System Mean:   {results['system_mean']:.4f}\")\n",
        "# print(f\"  Difference:    {results['mean_difference']:.4f}\")\n",
        "# print(f\"  p-value:       {results['p_value']:.4f}\")\n",
        "# print(f\"  95% CI:         [{results['ci_lower']:.4f}, {results['ci_upper']:.4f}]\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
