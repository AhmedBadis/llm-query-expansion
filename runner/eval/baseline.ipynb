{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba84f52",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "\n",
    "This notebook evaluates baseline retrieval methods (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required DUMMY files will be created automatically if missing\n",
    "3. Results will be saved to `output/eval/metric/baseline/`\n",
    "4. Plots will be saved to `output/eval/plot/baseline/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd87e771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook.run_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     compute_metrics_from_files,\n\u001b[32m     15\u001b[39m     save_metrics_to_csv,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     compare_runs,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     ensure_directory,\n\u001b[32m     23\u001b[39m     create_summary_table,\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnotebook\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure_baseline_runs\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSetup complete! Project root:\u001b[39m\u001b[33m\"\u001b[39m, project_root)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'notebook.run_api'"
     ]
    }
   ],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is located at runner/eval/baseline.ipynb\n",
    "project_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from eval import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_per_query_metric,\n",
    "    compare_runs,\n",
    ")\n",
    "from eval.utils import (\n",
    "    ensure_directory,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_baseline_runs\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c774fd",
   "metadata": {},
   "source": [
    "## Ensure ingest outputs and baseline runs\n",
    "\n",
    "Use the programmatic ingest + retrieval API to materialize real artifacts under `output/ingest/{dataset}` and `output/retrieval/baseline/` if they are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ee474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core configuration\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\"]  # baseline retrieval methods we actually support\n",
    "output_base = project_root / \"output\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "ensure_directory(output_base / \"retrieval\" / \"baseline\")\n",
    "ensure_directory(output_base / \"eval\" / \"metric\" / \"baseline\")\n",
    "ensure_directory(output_base / \"eval\" / \"plot\")\n",
    "ensure_directory(output_base / \"eval\" / \"slice\")\n",
    "\n",
    "# Ensure ingest artifacts + baseline runs (BM25) exist\n",
    "runs = ensure_baseline_runs(datasets=datasets, retrieval_methods=retrieval_methods, top_k=100)\n",
    "print(\"Baseline runs ensured:\\n\", runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862996fd",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n",
    "\n",
    "Compute nDCG@10, MAP, Recall@100, MRR for:\n",
    "- BM25 × TREC-COVID\n",
    "- BM25 × Climate-Fever  \n",
    "- TF-IDF × TREC-COVID\n",
    "- TF-IDF × Climate-Fever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732097b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for all combinations using real ingest outputs\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = output_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = output_base / \"retrieval\" / \"baseline\" / f\"{retrieval}_{dataset}.csv\"\n",
    "        metric_path = output_base / \"eval\" / \"metric\" / \"baseline\" / f\"{retrieval}_{dataset}.csv\"\n",
    "\n",
    "        # Compute and save metrics\n",
    "        metrics = compute_metrics_from_files(str(run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(metric_path),\n",
    "            dataset=dataset,\n",
    "            method=\"baseline\",\n",
    "            retrieval=retrieval,\n",
    "        )\n",
    "\n",
    "        all_metrics[(dataset, \"baseline\", retrieval)] = metrics\n",
    "        print(f\"{retrieval} × {dataset}: nDCG@10={metrics['ndcg@10']:.4f}, MAP={metrics['map']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0ee3",
   "metadata": {},
   "source": [
    "## Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ac61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = output_base / \"eval\" / \"metric\" / \"baseline\" / \"summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76976cd",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nDCG@10 comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.35\n",
    "\n",
    "bm25_scores = [all_metrics[(d, \"baseline\", \"bm25\")][\"ndcg@10\"] for d in datasets]\n",
    "\n",
    "bars1 = ax.bar(x, bm25_scores, width, label=\"BM25\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "ax.set_ylabel(\"nDCG@10\", fontsize=12)\n",
    "ax.set_title(\"Baseline nDCG@10 (BM25)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "            f\"{height:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = output_base / \"eval\" / \"plot\" / \"baseline_ndcg_bm25.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f42c6d",
   "metadata": {},
   "source": [
    "## Robustness Analysis: Compute Query Slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016724e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute query slices for each dataset using ingest outputs (queries + vocab)\n",
    "from eval.robustness_slices import compute_query_slices, load_vocabulary\n",
    "\n",
    "for dataset in datasets:\n",
    "    run_path = output_base / \"retrieval\" / \"baseline\" / f\"bm25_{dataset}.csv\"\n",
    "    vocab_path = output_base / \"ingest\" / dataset / \"vocab_top50k.txt\"\n",
    "    queries_path = output_base / \"ingest\" / dataset / \"queries.csv\"\n",
    "    slices_path = output_base / \"eval\" / \"slice\" / f\"{dataset}.csv\"\n",
    "\n",
    "    # Load vocabulary\n",
    "    vocab = load_vocabulary(str(vocab_path), top_n=50000)\n",
    "\n",
    "    # Load queries.csv (query_id,text,split)\n",
    "    queries_df = pd.read_csv(queries_path)\n",
    "    queries = {row[\"query_id\"]: row[\"text\"] for _, row in queries_df.iterrows()}\n",
    "\n",
    "    # Compute slices\n",
    "    slices = compute_query_slices(\n",
    "        queries,\n",
    "        str(run_path),\n",
    "        vocab=vocab,\n",
    "        output_file=str(slices_path),\n",
    "    )\n",
    "\n",
    "    familiar = sum(1 for s in slices.values() if s[\"label\"] == \"familiar\")\n",
    "    unfamiliar = len(slices) - familiar\n",
    "\n",
    "    print(f\"{dataset}: {familiar} familiar, {unfamiliar} unfamiliar queries\")\n",
    "    print(f\"Slices saved to: {slices_path}\")\n",
    "\n",
    "print(\"\\nRobustness analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
