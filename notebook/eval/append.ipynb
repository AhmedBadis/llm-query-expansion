{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68e05b1",
   "metadata": {},
   "source": [
    "# Append Method Evaluation\n",
    "\n",
    "This notebook evaluates the Append query expansion method (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets, comparing against baseline.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required files will be created automatically if missing\n",
    "3. Metrics and plots will be saved to `data/eval/append/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dece4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is located at runner/eval/append.ipynb\n",
    "project_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from eval import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_per_query_metric,\n",
    "    compare_runs,\n",
    ")\n",
    "from eval.utils import (\n",
    "    ensure_directory,\n",
    "    find_top_delta_queries,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_baseline_runs, run_method\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51210cd6",
   "metadata": {},
   "source": [
    "## Ensure DUMMY Files Exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\"]  # we only have a BM25 baseline\n",
    "method_name = \"append\"\n",
    "output_base = project_root / \"data\"\n",
    "\n",
    "# Ensure baseline runs (BM25) exist so we can compare against them\n",
    "ensure_baseline_runs(datasets=datasets, retrieval_methods=[\"bm25\"], top_k=100)\n",
    "\n",
    "# Create directory structure for method runs\n",
    "ensure_directory(output_base / \"retrieval\" / method_name)\n",
    "ensure_directory(output_base / \"eval\" / \"metric\" / method_name)\n",
    "\n",
    "# Create dummy append run files for the method if missing\n",
    "for dataset in datasets:\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = output_base / \"retrieval\" / method_name / f\"{retrieval}_{dataset}_DUMMY.csv\"\n",
    "        if not run_path.exists():\n",
    "            from eval.utils import create_dummy_run_file\n",
    "            create_dummy_run_file(str(run_path), num_queries=10, num_docs_per_query=100)\n",
    "            print(f\"Created dummy run: {run_path}\")\n",
    "\n",
    "print(\"Append DUMMY method runs check complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4cf99",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58457413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for append method\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = output_base / 'ingest' / dataset / 'qrels.csv'\n",
    "    \n",
    "    for retrieval in retrieval_methods:\n",
    "        # Append run\n",
    "        append_run_path = output_base / 'retrieval' / method_name / f'{retrieval}_{dataset}_DUMMY.csv'\n",
    "        append_metric_path = output_base / 'eval' / 'metric' / method_name / f'{retrieval}_{dataset}_DUMMY.csv'\n",
    "        \n",
    "        # Baseline run for comparison\n",
    "        baseline_run_path = output_base / 'retrieval' / 'baseline' / f'{retrieval}_{dataset}_DUMMY.csv'\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics_from_files(str(append_run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(append_metric_path),\n",
    "            dataset=dataset,\n",
    "            method=method_name,\n",
    "            retrieval=retrieval\n",
    "        )\n",
    "        \n",
    "        all_metrics[(dataset, method_name, retrieval)] = metrics\n",
    "        print(f\"{retrieval} × {dataset}: nDCG@10={metrics['ndcg@10']:.4f}, MAP={metrics['map']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d796d9",
   "metadata": {},
   "source": [
    "## Compare with Baseline (Statistical Tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare append method with baseline\n",
    "comparison_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = output_base / 'ingest' / dataset / 'qrels.csv'\n",
    "    \n",
    "    for retrieval in retrieval_methods:\n",
    "        append_run_path = output_base / \"retrieval\" / method_name / f\"{retrieval}_{dataset}_DUMMY.csv\"\n",
    "        baseline_run_path = output_base / \"retrieval\" / \"baseline\" / f\"{retrieval}_{dataset}.csv\"\n",
    "        \n",
    "        # Statistical comparison\n",
    "        stats = compare_runs(\n",
    "            str(baseline_run_path),\n",
    "            str(append_run_path),\n",
    "            str(qrels_path),\n",
    "            metric='ndcg@10',\n",
    "            k=10\n",
    "        )\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'dataset': dataset,\n",
    "            'retrieval': retrieval,\n",
    "            'baseline_mean': stats['baseline_mean'],\n",
    "            'append_mean': stats['system_mean'],\n",
    "            'difference': stats['mean_difference'],\n",
    "            'p_value': stats['p_value'],\n",
    "            'ci_lower': stats['ci_lower'],\n",
    "            'ci_upper': stats['ci_upper']\n",
    "        })\n",
    "        \n",
    "        print(f\"{retrieval} × {dataset}: Δ={stats['mean_difference']:.4f}, p={stats['p_value']:.4f}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save p-values and CIs to JSON\n",
    "import json\n",
    "pvals_path = output_base / \"eval\" / \"metric\" / method_name / \"pvals_DUMMY.json\"\n",
    "ensure_directory(pvals_path.parent)\n",
    "pvals_data = {\n",
    "    'method': method_name,\n",
    "    'comparisons': comparison_results\n",
    "}\n",
    "with open(pvals_path, 'w') as f:\n",
    "    json.dump(pvals_data, f, indent=2)\n",
    "print(f\"\\nP-values and CIs saved to: {pvals_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884d8b1",
   "metadata": {},
   "source": [
    "## Summary Table and Top Delta Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23640c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = output_base / 'eval' / 'metric' / method_name / f'summary_DUMMY.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Show top delta queries for one example\n",
    "dataset_example = datasets[0]\n",
    "retrieval_example = retrieval_methods[0]\n",
    "\n",
    "qrels = load_qrels_file(str(output_base / 'ingest' / dataset_example / 'qrels.csv'))\n",
    "baseline_run = load_run_file(str(output_base / \"retrieval\" / \"baseline\" / f\"{retrieval_example}_{dataset_example}.csv\"))\n",
    "append_run = load_run_file(str(output_base / \"retrieval\" / method_name / f\"{retrieval_example}_{dataset_example}_DUMMY.csv\"))\n",
    "\n",
    "baseline_scores = compute_per_query_metric(baseline_run, qrels, metric='ndcg@10', k=10)\n",
    "append_scores = compute_per_query_metric(append_run, qrels, metric='ndcg@10', k=10)\n",
    "\n",
    "top_positive, top_negative = find_top_delta_queries(baseline_scores, append_scores, top_n=10)\n",
    "\n",
    "print(f\"\\nTop 10 Positive Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_positive[:5]:\n",
    "    print(f\"  {qid}: +{delta:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 10 Negative Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_negative[:5]:\n",
    "    print(f\"  {qid}: {delta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f384f",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56367418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline metrics for comparison\n",
    "from eval.compute_metrics import compute_metrics_from_files as load_metrics\n",
    "\n",
    "baseline_metrics = {}\n",
    "for dataset in datasets:\n",
    "    for retrieval in retrieval_methods:\n",
    "        qrels_path = output_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "        baseline_run_path = output_base / \"retrieval\" / \"baseline\" / f\"{retrieval}_{dataset}.csv\"\n",
    "        baseline_metrics[(dataset, retrieval)] = load_metrics(str(baseline_run_path), str(qrels_path), k=10)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Append Method vs Baseline: nDCG@10', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, retrieval in enumerate(retrieval_methods):\n",
    "    ax = axes[idx]\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    baseline_scores = [baseline_metrics[(d, retrieval)]['ndcg@10'] for d in datasets]\n",
    "    append_scores = [all_metrics[(d, method_name, retrieval)]['ndcg@10'] for d in datasets]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, append_scores, width, label='Append', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Dataset', fontsize=12)\n",
    "    ax.set_ylabel('nDCG@10', fontsize=12)\n",
    "    ax.set_title(f'{retrieval.upper()}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# TODO: data/eval/plot/{method}_{retrieval}_ndcg.png\n",
    "plot_path = output_base / 'eval' / 'plot' / f'{method_name}_ndcg_DUMMY.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
