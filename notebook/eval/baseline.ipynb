{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba84f52",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "\n",
    "This notebook evaluates baseline retrieval methods (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required DUMMY files will be created automatically if missing\n",
    "3. Results will be saved to `output/eval/metric/baseline/`\n",
    "4. Plots will be saved to `output/eval/plot/baseline/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd87e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Project root: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\.venv\\Lib\\site-packages\\beir\\datasets\\data_loader.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Notebook is located at runner/eval/baseline.ipynb\n",
    "project_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from eval import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_per_query_metric,\n",
    "    compare_runs,\n",
    ")\n",
    "from eval.utils import (\n",
    "    ensure_directory,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_baseline_runs\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c774fd",
   "metadata": {},
   "source": [
    "## Ensure ingest outputs and baseline runs\n",
    "\n",
    "Use the programmatic ingest + retrieval API to materialize real artifacts under `output/ingest/{dataset}` and `output/retrieval/baseline/` if they are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ee474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: trec_covid ===\n",
      "Loaded ingested dataset 'trec_covid' from C:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\output\\ingest\n",
      "[trec_covid / bm25] Baseline run already exists at C:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\output\\retrieval\\baseline\\bm25_trec_covid.csv\n",
      "\n",
      "=== Dataset: climate_fever ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m ensure_directory(output_base / \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mslice\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Ensure ingest artifacts + baseline runs (BM25) exist\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m runs = \u001b[43mensure_baseline_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval_methods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretrieval_methods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBaseline runs ensured:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, runs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\src\\notebook\\run_api.py:87\u001b[39m, in \u001b[36mensure_baseline_runs\u001b[39m\u001b[34m(datasets, retrieval_methods, top_k)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Ensure ingest artifacts exist\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     corpus, queries, _ = \u001b[43mload_ingested_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mingested_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINGESTED_ROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded ingested dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINGESTED_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\src\\ingest\\core.py:152\u001b[39m, in \u001b[36mload_ingested_dataset\u001b[39m\u001b[34m(dataset, ingested_root, load_tokenized, tokenized_filename)\u001b[39m\n\u001b[32m    149\u001b[39m     tokenized_docs = _load_tokenized_docs(tokenized_path, dataset)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m paths.docs.open(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[32m    324\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.buffer + \u001b[38;5;28minput\u001b[39m\n\u001b[32m    325\u001b[39m     (result, consumed) = \u001b[38;5;28mself\u001b[39m._buffer_decode(data, \u001b[38;5;28mself\u001b[39m.errors, final)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define core configuration\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\"]  # baseline retrieval methods we actually support\n",
    "output_base = project_root / \"output\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "ensure_directory(output_base / \"retrieval\" / \"baseline\")\n",
    "ensure_directory(output_base / \"eval\" / \"metric\" / \"baseline\")\n",
    "ensure_directory(output_base / \"eval\" / \"plot\")\n",
    "ensure_directory(output_base / \"eval\" / \"slice\")\n",
    "\n",
    "# Ensure ingest artifacts + baseline runs (BM25) exist\n",
    "runs = ensure_baseline_runs(datasets=datasets, retrieval_methods=retrieval_methods, top_k=100)\n",
    "print(\"Baseline runs ensured:\\n\", json.dumps(runs, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862996fd",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n",
    "\n",
    "Compute nDCG@10, MAP, Recall@100, MRR for:\n",
    "- BM25 × TREC-COVID\n",
    "- BM25 × Climate-Fever  \n",
    "- TF-IDF × TREC-COVID\n",
    "- TF-IDF × Climate-Fever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732097b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute metrics for all combinations using real ingest outputs\u001b[39;00m\n\u001b[32m      2\u001b[39m all_metrics = {}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatasets\u001b[49m:\n\u001b[32m      5\u001b[39m     qrels_path = output_base / \u001b[33m\"\u001b[39m\u001b[33mingest\u001b[39m\u001b[33m\"\u001b[39m / dataset / \u001b[33m\"\u001b[39m\u001b[33mqrels.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m retrieval \u001b[38;5;129;01min\u001b[39;00m retrieval_methods:\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute metrics for all combinations using real ingest outputs\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = output_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = output_base / \"retrieval\" / \"baseline\" / f\"{retrieval}_{dataset}.csv\"\n",
    "        metric_path = output_base / \"eval\" / \"metric\" / \"baseline\" / f\"{retrieval}_{dataset}.csv\"\n",
    "\n",
    "        # Compute and save metrics\n",
    "        metrics = compute_metrics_from_files(str(run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(metric_path),\n",
    "            dataset=dataset,\n",
    "            method=\"baseline\",\n",
    "            retrieval=retrieval,\n",
    "        )\n",
    "\n",
    "        all_metrics[(dataset, \"baseline\", retrieval)] = metrics\n",
    "        print(f\"{retrieval} × {dataset}: nDCG@10={metrics['ndcg@10']:.4f}, MAP={metrics['map']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0ee3",
   "metadata": {},
   "source": [
    "## Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ac61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = output_base / \"eval\" / \"metric\" / \"baseline\" / \"summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76976cd",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nDCG@10 comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.35\n",
    "\n",
    "bm25_scores = [all_metrics[(d, \"baseline\", \"bm25\")][\"ndcg@10\"] for d in datasets]\n",
    "\n",
    "bars1 = ax.bar(x, bm25_scores, width, label=\"BM25\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "ax.set_ylabel(\"nDCG@10\", fontsize=12)\n",
    "ax.set_title(\"Baseline nDCG@10 (BM25)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "            f\"{height:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = output_base / \"eval\" / \"plot\" / \"baseline_ndcg_bm25.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f42c6d",
   "metadata": {},
   "source": [
    "## Robustness Analysis: Compute Query Slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016724e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute query slices for each dataset using ingest outputs (queries + vocab)\n",
    "from eval.robustness_slices import compute_query_slices, load_vocabulary\n",
    "\n",
    "for dataset in datasets:\n",
    "    run_path = output_base / \"retrieval\" / \"baseline\" / f\"bm25_{dataset}.csv\"\n",
    "    vocab_path = output_base / \"ingest\" / dataset / \"vocab_top50k.txt\"\n",
    "    queries_path = output_base / \"ingest\" / dataset / \"queries.csv\"\n",
    "    slices_path = output_base / \"eval\" / \"slice\" / f\"{dataset}.csv\"\n",
    "\n",
    "    # Load vocabulary\n",
    "    vocab = load_vocabulary(str(vocab_path), top_n=50000)\n",
    "\n",
    "    # Load queries.csv (query_id,text,split)\n",
    "    queries_df = pd.read_csv(queries_path)\n",
    "    queries = {row[\"query_id\"]: row[\"text\"] for _, row in queries_df.iterrows()}\n",
    "\n",
    "    # Compute slices\n",
    "    slices = compute_query_slices(\n",
    "        queries,\n",
    "        str(run_path),\n",
    "        vocab=vocab,\n",
    "        output_file=str(slices_path),\n",
    "    )\n",
    "\n",
    "    familiar = sum(1 for s in slices.values() if s[\"label\"] == \"familiar\")\n",
    "    unfamiliar = len(slices) - familiar\n",
    "\n",
    "    print(f\"{dataset}: {familiar} familiar, {unfamiliar} unfamiliar queries\")\n",
    "    print(f\"Slices saved to: {slices_path}\")\n",
    "\n",
    "print(\"\\nRobustness analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
