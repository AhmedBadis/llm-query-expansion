{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcab749",
   "metadata": {},
   "source": [
    "# Reformulate Method Evaluation\n",
    "\n",
    "This notebook evaluates the Reformulate query expansion method (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets, comparing against baseline.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required files will be created automatically if missing\n",
    "3. Metrics and plots will be saved to `data/eval/reformulate/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is located at notebook/eval/reformulate.ipynb\n",
    "project_root = Path.cwd().parents[1]\n",
    "\n",
    "# Define core configuration\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\", \"tfidf\"]\n",
    "method_name = \"reformulate\"\n",
    "data_base = project_root / \"data\"\n",
    "\n",
    "auto_run_max_queries = None  # set to an int (e.g. 50) to limit Groq calls\n",
    "\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from eval import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_per_query_metric,\n",
    "    compare_runs,\n",
    ")\n",
    "from eval.utils import (\n",
    "    ensure_directory,\n",
    "    find_top_delta_queries,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_baseline_runs, ensure_method_runs\n",
    "\n",
    "from llm_qe.expander import GroqQueryExpander, ExpansionStrategy\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f04a8",
   "metadata": {},
   "source": [
    "## Ensure ingest outputs and method runs\n",
    "\n",
    "This will:\n",
    "- Ensure ingested artifacts exist under `data/ingest/{dataset}`\n",
    "- Ensure baseline runs exist under `data/retrieval/baseline/`\n",
    "- Expand the real dataset queries using Groq\n",
    "- Run retrieval (BM25 + TF-IDF) for the expanded queries and save to `data/retrieval/reformulate/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed27fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directory(data_base / \"retrieval\" / method_name)\n",
    "ensure_directory(data_base / \"eval\" / method_name)\n",
    "\n",
    "baseline_runs = ensure_baseline_runs(datasets=datasets, retrieval_methods=retrieval_methods, top_k=100)\n",
    "print(\"Baseline runs ensured:\\n\", json.dumps(baseline_runs, indent=2, default=str))\n",
    "\n",
    "expander = GroqQueryExpander(strategy=ExpansionStrategy.REFORMULATE)\n",
    "\n",
    "method_runs = ensure_method_runs(\n",
    "    method_name=method_name,\n",
    "    strategy=ExpansionStrategy.REFORMULATE,\n",
    "    expander=expander,\n",
    "    datasets=datasets,\n",
    "    retrieval_methods=retrieval_methods,\n",
    "    top_k=100,\n",
    "    max_queries=auto_run_max_queries,\n",
    ")\n",
    "print(f\"{method_name} runs ensured:\\n\", json.dumps(method_runs, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770c9c8",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132feba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for reformulate method runs (real data)\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieval\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "        metric_path = data_base / \"eval\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        metrics = compute_metrics_from_files(str(run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(metric_path),\n",
    "            dataset=dataset,\n",
    "            method=method_name,\n",
    "            retrieval=retrieval,\n",
    "        )\n",
    "\n",
    "        all_metrics[(dataset, method_name, retrieval)] = metrics\n",
    "        print(f\"{retrieval} × {dataset}: nDCG@10={metrics['ndcg@10']:.4f}, MAP={metrics['map']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be0df6",
   "metadata": {},
   "source": [
    "## Compare with Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a09cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare reformulate method with baseline (statistical test on nDCG@10)\n",
    "comparison_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieval\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "        baseline_run_path = data_base / \"retrieval\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        stats = compare_runs(\n",
    "            str(baseline_run_path),\n",
    "            str(run_path),\n",
    "            str(qrels_path),\n",
    "            metric=\"ndcg@10\",\n",
    "            k=10,\n",
    "        )\n",
    "\n",
    "        comparison_results.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"retrieval\": retrieval,\n",
    "                \"baseline_mean\": stats[\"baseline_mean\"],\n",
    "                \"reformulate_mean\": stats[\"system_mean\"],\n",
    "                \"difference\": stats[\"mean_difference\"],\n",
    "                \"p_value\": stats[\"p_value\"],\n",
    "                \"ci_lower\": stats.get(\"ci_lower\"),\n",
    "                \"ci_upper\": stats.get(\"ci_upper\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"{retrieval} × {dataset}: Δ={stats['mean_difference']:.4f}, p={stats['p_value']:.4f}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "pvals_path = data_base / \"eval\" / method_name / \"pvals.json\"\n",
    "ensure_directory(pvals_path.parent)\n",
    "with open(pvals_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"method\": method_name, \"comparisons\": comparison_results}, f, indent=2)\n",
    "print(f\"\\nP-values and CIs saved to: {pvals_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9916a",
   "metadata": {},
   "source": [
    "## Summary and Top Delta Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf614fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = data_base / \"eval\" / method_name / \"summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved to: {summary_path}\")\n",
    "\n",
    "# Show top delta queries for one example\n",
    "dataset_example = datasets[0]\n",
    "retrieval_example = retrieval_methods[0]\n",
    "\n",
    "qrels = load_qrels_file(str(data_base / \"ingest\" / dataset_example / \"qrels.csv\"))\n",
    "baseline_run = load_run_file(str(data_base / \"retrieval\" / \"baseline\" / f\"{dataset_example}_{retrieval_example}.csv\"))\n",
    "method_run = load_run_file(str(data_base / \"retrieval\" / method_name / f\"{dataset_example}_{retrieval_example}.csv\"))\n",
    "\n",
    "baseline_scores = compute_per_query_metric(baseline_run, qrels, metric=\"ndcg@10\", k=10)\n",
    "method_scores = compute_per_query_metric(method_run, qrels, metric=\"ndcg@10\", k=10)\n",
    "\n",
    "top_positive, top_negative = find_top_delta_queries(baseline_scores, method_scores, top_n=10)\n",
    "\n",
    "print(f\"\\nTop 5 Positive Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_positive[:5]:\n",
    "    print(f\"  {qid}: +{delta:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 5 Negative Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_negative[:5]:\n",
    "    print(f\"  {qid}: {delta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d763fa",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nDCG@10 comparison (baseline vs reformulate)\n",
    "from eval.compute_metrics import compute_metrics_from_files as load_metrics\n",
    "\n",
    "baseline_metrics = {}\n",
    "for dataset in datasets:\n",
    "    for retrieval in retrieval_methods:\n",
    "        qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "        baseline_run_path = data_base / \"retrieval\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "        baseline_metrics[(dataset, retrieval)] = load_metrics(str(baseline_run_path), str(qrels_path), k=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(retrieval_methods), figsize=(14, 6), squeeze=False)\n",
    "fig.suptitle(\"Reformulate Method vs Baseline: nDCG@10\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "for idx, retrieval in enumerate(retrieval_methods):\n",
    "    ax = axes[0][idx]\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "\n",
    "    baseline_scores = [baseline_metrics[(d, retrieval)][\"ndcg@10\"] for d in datasets]\n",
    "    method_scores = [all_metrics[(d, method_name, retrieval)][\"ndcg@10\"] for d in datasets]\n",
    "\n",
    "    bars1 = ax.bar(x - width / 2, baseline_scores, width, label=\"Baseline\", alpha=0.8)\n",
    "    bars2 = ax.bar(x + width / 2, method_scores, width, label=\"Reformulate\", alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "    ax.set_ylabel(\"nDCG@10\", fontsize=12)\n",
    "    ax.set_title(f\"{retrieval.upper()}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = data_base / \"eval\" / method_name / \"ndcg.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
