{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba84f52",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "\n",
    "This notebook evaluates baseline retrieval methods (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required files will be created automatically if missing\n",
    "3. Metrics and plots will be saved to `data/evaluate/baseline/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd87e771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:31:11.797750Z",
     "iopub.status.busy": "2025-12-11T01:31:11.797501Z",
     "iopub.status.idle": "2025-12-11T01:31:13.582837Z",
     "shell.execute_reply": "2025-12-11T01:31:13.581999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Project root: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\.venv\\Lib\\site-packages\\beir\\datasets\\data_loader.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Notebook is located at notebook/baseline.ipynb\n",
    "project_root = Path.cwd().parents[0]\n",
    "\n",
    "# Define core configuration\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\", \"tfidf\"]  # baseline retrieval methods we actually support\n",
    "data_base = project_root / \"data\"\n",
    "\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from evaluate import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    ")\n",
    "from evaluate.utils import (\n",
    "    ensure_directory,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_runs\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c774fd",
   "metadata": {},
   "source": [
    "## Ensure ingest outputs and baseline runs\n",
    "\n",
    "Use the programmatic ingest + retrieval API to materialize real artifacts under data/ingest/{dataset} and data/retrieve/baseline/ if they are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61ee474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:31:13.585700Z",
     "iopub.status.busy": "2025-12-11T01:31:13.585426Z",
     "iopub.status.idle": "2025-12-11T01:35:17.351458Z",
     "shell.execute_reply": "2025-12-11T01:35:17.347024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset: trec_covid ===\n",
      "[trec_covid] Ingested artifacts are stale (newer extract found); will re-ingest.\n",
      "[trec_covid] Extracted dataset is stale (newer download found); will re-extract.\n",
      "[trec_covid] Re-extracting from existing download...\n",
      "Downloading trec_covid (BEIR name: trec-covid) from https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip ...\n",
      "Downloaded and renamed to C:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\download\\trec_covid.zip\n",
      "Extracting C:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\download\\trec_covid.zip to C:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\extract\\trec_covid ...\n",
      "Finished extracting to C:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\extract\\trec_covid\n",
      "Extracted trec_covid\n",
      "[trec_covid] Ingesting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4633e34b2eb45bfab7bd1160257b8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested trec_covid\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m ensure_directory(data_base / \u001b[33m\"\u001b[39m\u001b[33mevaluate\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Ensure baseline runs exist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m runs = \u001b[43mensure_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbaseline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval_methods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretrieval_methods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBaseline runs ensured:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, json.dumps(runs, indent=\u001b[32m2\u001b[39m, default=\u001b[38;5;28mstr\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\src\\notebook\\run_api.py:549\u001b[39m, in \u001b[36mensure_runs\u001b[39m\u001b[34m(method, expander, datasets, retrieval_methods, top_k, max_queries, model_name, api_key, overwrite_expansions)\u001b[39m\n\u001b[32m    546\u001b[39m manager.ensure_tokenized()\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m# 6) Merge tokenized into corpus for faster retrieval\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_tokenized_into_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# 7) Generate runs\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\src\\notebook\\run_api.py:388\u001b[39m, in \u001b[36mRunManager.load_tokenized_into_corpus\u001b[39m\u001b[34m(self, corpus)\u001b[39m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     tokenized_corpus = \u001b[43mload_tokenized_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, doc_data \u001b[38;5;129;01min\u001b[39;00m corpus.items():\n\u001b[32m    390\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m tokenized_corpus:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\src\\index\\tokenize.py:197\u001b[39m, in \u001b[36mload_tokenized_corpus\u001b[39m\u001b[34m(dataset, ingested_root, output_filename)\u001b[39m\n\u001b[32m    195\u001b[39m corpus: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m]] = {}\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m output_path.open(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[32m    324\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.buffer + \u001b[38;5;28minput\u001b[39m\n\u001b[32m    325\u001b[39m     (result, consumed) = \u001b[38;5;28mself\u001b[39m._buffer_decode(data, \u001b[38;5;28mself\u001b[39m.errors, final)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "ensure_directory(data_base / \"retrieve\" / \"baseline\")\n",
    "ensure_directory(data_base / \"evaluate\" / \"baseline\")\n",
    "\n",
    "# Ensure baseline runs exist\n",
    "runs = ensure_runs(method=\"baseline\", datasets=datasets, retrieval_methods=retrieval_methods, top_k=100)\n",
    "print(\"Baseline runs ensured:\\n\", json.dumps(runs, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cde5a",
   "metadata": {},
   "source": [
    "## Load and display dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078fd2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:35:17.413506Z",
     "iopub.status.busy": "2025-12-11T01:35:17.412294Z",
     "iopub.status.idle": "2025-12-11T01:35:17.452561Z",
     "shell.execute_reply": "2025-12-11T01:35:17.451407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and display dataset statistics from manifest.json\n",
    "print(\"\\n=== Dataset Statistics ===\")\n",
    "from ingest.core import get_ingested_dataset_paths\n",
    "\n",
    "dataset_stats = {}\n",
    "for dataset in datasets:\n",
    "    paths = get_ingested_dataset_paths(dataset)\n",
    "    if paths.manifest.exists():\n",
    "        manifest_data = json.loads(paths.manifest.read_text(encoding=\"utf-8\"))\n",
    "        dataset_stats[dataset] = {\n",
    "            \"doc_count\": manifest_data.get(\"doc_count\", \"N/A\"),\n",
    "            \"query_count\": manifest_data.get(\"query_count\", \"N/A\"),\n",
    "            \"qrels_count\": manifest_data.get(\"qrels_count\", \"N/A\"),\n",
    "            \"vocab_size\": manifest_data.get(\"vocab_size\", \"N/A\"),\n",
    "            \"split\": manifest_data.get(\"split\", \"N/A\"),\n",
    "        }\n",
    "        print(f\"{dataset}:\")\n",
    "        print(f\"  Documents: {dataset_stats[dataset]['doc_count']}\")\n",
    "        print(f\"  Queries: {dataset_stats[dataset]['query_count']}\")\n",
    "        print(f\"  Qrels: {dataset_stats[dataset]['qrels_count']}\")\n",
    "        print(f\"  Vocab size: {dataset_stats[dataset]['vocab_size']}\")\n",
    "        print(f\"  Split: {dataset_stats[dataset]['split']}\")\n",
    "\n",
    "# Save dataset statistics to data/evaluate\n",
    "if dataset_stats:\n",
    "    stats_path = data_base / \"evaluate\" / \"dataset_stats.json\"\n",
    "    ensure_directory(stats_path.parent)\n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset_stats, f, indent=2)\n",
    "    print(f\"\\nDataset statistics saved to: {stats_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862996fd",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n",
    "\n",
    "Compute nDCG@10, MAP, Recall@100, MRR for:\n",
    "- BM25 × TREC-COVID\n",
    "- BM25 × Climate-Fever  \n",
    "- TF-IDF × TREC-COVID\n",
    "- TF-IDF × Climate-Fever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732097b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:35:17.458188Z",
     "iopub.status.busy": "2025-12-11T01:35:17.457954Z",
     "iopub.status.idle": "2025-12-11T01:35:18.037884Z",
     "shell.execute_reply": "2025-12-11T01:35:18.035043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute metrics for all combinations using real ingest outputs\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieve\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "        metric_path = data_base / \"evaluate\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        # Compute and save metrics\n",
    "        metrics = compute_metrics_from_files(str(run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(metric_path),\n",
    "            dataset=dataset,\n",
    "            method=\"baseline\",\n",
    "            retrieval=retrieval,\n",
    "        )\n",
    "\n",
    "        all_metrics[(dataset, \"baseline\", retrieval)] = metrics\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0ee3",
   "metadata": {},
   "source": [
    "## Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ac61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:35:18.041951Z",
     "iopub.status.busy": "2025-12-11T01:35:18.041749Z",
     "iopub.status.idle": "2025-12-11T01:35:18.088734Z",
     "shell.execute_reply": "2025-12-11T01:35:18.086715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and display summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = data_base / \"evaluate\" / \"baseline\" / \"summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76976cd",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649ecde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:35:18.092464Z",
     "iopub.status.busy": "2025-12-11T01:35:18.092246Z",
     "iopub.status.idle": "2025-12-11T01:35:18.618939Z",
     "shell.execute_reply": "2025-12-11T01:35:18.617733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot nDCG@10 comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.35\n",
    "\n",
    "bm25_scores = [all_metrics[(d, \"baseline\", \"bm25\")][\"ndcg@10\"] for d in datasets]\n",
    "tfidf_scores = [all_metrics[(d, \"baseline\", \"tfidf\")][\"ndcg@10\"] for d in datasets]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, bm25_scores, width, label=\"BM25\", alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, tfidf_scores, width, label=\"TF-IDF\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "ax.set_ylabel(\"nDCG@10\", fontsize=12)\n",
    "ax.set_title(\"Baseline nDCG@10 (BM25 vs TF-IDF)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "                f\"{height:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = data_base / \"evaluate\" / \"baseline\" / \"ndcg.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f42c6d",
   "metadata": {},
   "source": [
    "## Robustness Analysis: Compute Query Slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016724e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T01:35:18.628356Z",
     "iopub.status.busy": "2025-12-11T01:35:18.628157Z",
     "iopub.status.idle": "2025-12-11T01:35:19.642419Z",
     "shell.execute_reply": "2025-12-11T01:35:19.641574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute query slices for each dataset using ingest outputs (queries + vocab)\n",
    "from evaluate.robustness_slices import compute_query_slices, load_vocabulary\n",
    "\n",
    "for dataset in datasets:\n",
    "    vocab_path = data_base / \"ingest\" / dataset / \"vocab_top50k.txt\"\n",
    "    queries_path = data_base / \"ingest\" / dataset / \"queries.csv\"\n",
    "    slices_path = data_base / \"evaluate\" / f\"{dataset}.csv\"\n",
    "\n",
    "    # Load vocabulary\n",
    "    vocab = load_vocabulary(str(vocab_path), top_n=50000)\n",
    "\n",
    "    # Load queries.csv (query_id,text,split)\n",
    "    queries_df = pd.read_csv(queries_path)\n",
    "    # Convert query_id to string to match run file format\n",
    "    queries = {str(row[\"query_id\"]): row[\"text\"] for _, row in queries_df.iterrows()}\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieve\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "        # Compute slices\n",
    "        slices = compute_query_slices(\n",
    "            queries,\n",
    "            str(run_path),\n",
    "            vocab=vocab,\n",
    "            output_file=str(slices_path),\n",
    "        )\n",
    "\n",
    "    familiar = sum(1 for s in slices.values() if s[\"label\"] == \"familiar\")\n",
    "    unfamiliar = len(slices) - familiar\n",
    "\n",
    "    print(f\"{dataset}: {familiar} familiar, {unfamiliar} unfamiliar queries\")\n",
    "    print(f\"Slices saved to: {slices_path}\")\n",
    "\n",
    "print(\"\\nRobustness analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
