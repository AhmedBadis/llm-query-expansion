{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68e05b1",
   "metadata": {},
   "source": [
    "# Append Method Evaluation\n",
    "\n",
    "This notebook evaluates the Append query expansion method (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets, comparing against baseline.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required files will be created automatically if missing\n",
    "3. Metrics and plots will be saved to `data/evaluate/append/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dece4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\.venv\\Lib\\site-packages\\beir\\datasets\\data_loader.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Project root: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is located at notebook/append.ipynb\n",
    "project_root = Path.cwd().parents[0]\n",
    "\n",
    "# Define core configuration\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\", \"tfidf\"]\n",
    "method_name = \"append\"\n",
    "data_base = project_root / \"data\"\n",
    "\n",
    "auto_run_max_queries = None  # set to an int (e.g. 50) to limit Groq calls\n",
    "\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from evaluate import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_per_query_metric,\n",
    "    compare_runs,\n",
    ")\n",
    "from evaluate.utils import (\n",
    "    ensure_directory,\n",
    "    find_top_delta_queries,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_runs\n",
    "\n",
    "from expand.expander import TogetherQueryExpander, ExpansionStrategy\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51210cd6",
   "metadata": {},
   "source": [
    "## Ensure ingest outputs and method runs\n",
    "\n",
    "This will:\n",
    "- Ensure ingested artifacts exist under `data/ingest/{dataset}`\n",
    "- Ensure baseline runs exist under `data/retrieve/baseline/`\n",
    "- Expand the real dataset queries using Groq\n",
    "- Run retrieval (BM25 + TF-IDF) for the expanded queries and save to `data/retrieve/append/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset: trec_covid ===\n",
      "[trec_covid] All requested baseline bm25 runs are valid; skipping all upstream work.\n",
      "[trec_covid] All requested baseline tfidf runs are valid; skipping all upstream work.\n",
      "=== Dataset: climate_fever ===\n",
      "[climate_fever] All requested baseline bm25 runs are valid; skipping all upstream work.\n",
      "[climate_fever] All requested baseline tfidf runs are valid; skipping all upstream work.\n",
      "Baseline runs ensured:\n",
      " {\n",
      "  \"trec_covid\": {\n",
      "    \"bm25\": \"C:\\\\Users\\\\Lenovo\\\\CodeProjects\\\\VSC\\\\domain-specific-query-expansion-with-llms\\\\data\\\\retrieval\\\\baseline\\\\trec_covid_bm25.csv\",\n",
      "    \"tfidf\": \"C:\\\\Users\\\\Lenovo\\\\CodeProjects\\\\VSC\\\\domain-specific-query-expansion-with-llms\\\\data\\\\retrieval\\\\baseline\\\\trec_covid_tfidf.csv\"\n",
      "  },\n",
      "  \"climate_fever\": {\n",
      "    \"bm25\": \"C:\\\\Users\\\\Lenovo\\\\CodeProjects\\\\VSC\\\\domain-specific-query-expansion-with-llms\\\\data\\\\retrieval\\\\baseline\\\\climate_fever_bm25.csv\",\n",
      "    \"tfidf\": \"C:\\\\Users\\\\Lenovo\\\\CodeProjects\\\\VSC\\\\domain-specific-query-expansion-with-llms\\\\data\\\\retrieval\\\\baseline\\\\climate_fever_tfidf.csv\"\n",
      "  }\n",
      "}\n",
      "Strategy: append\n",
      "=== Dataset: trec_covid ===\n",
      "[trec_covid] All requested baseline bm25 runs are valid; skipping all upstream work.\n",
      "[trec_covid] All requested baseline tfidf runs are valid; skipping all upstream work.\n",
      "=== Dataset: climate_fever ===\n",
      "[climate_fever] All requested baseline bm25 runs are valid; skipping all upstream work.\n",
      "[climate_fever] All requested baseline tfidf runs are valid; skipping all upstream work.\n",
      "=== Dataset: trec_covid ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m expander = GroqQueryExpander(strategy=ExpansionStrategy.APPEND)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Ensure method runs exist\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m method_runs = \u001b[43mensure_method_runs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExpansionStrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPPEND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpander\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpander\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretrieval_methods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretrieval_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_run_max_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m runs ensured:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, json.dumps(method_runs, indent=\u001b[32m2\u001b[39m, default=\u001b[38;5;28mstr\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\src\\notebook\\run_api.py:462\u001b[39m, in \u001b[36mensure_method_runs\u001b[39m\u001b[34m(method_name, strategy, expander, datasets, retrieval_methods, top_k, max_queries, groq_model_name, api_key, overwrite_expansions)\u001b[39m\n\u001b[32m    460\u001b[39m             tokens = tokenized_corpus[doc_id].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    461\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m                 doc_data[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] = [\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(token)]\n\u001b[32m    463\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded pre-tokenized data for faster BM25 retrieval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ensure_directory(data_base / \"retrieve\" / method_name)\n",
    "ensure_directory(data_base / \"evaluate\" / method_name)\n",
    "\n",
    "# Ensure baseline runs exist\n",
    "baseline_runs = ensure_runs(method=\"baseline\", datasets=datasets, retrieval_methods=retrieval_methods, top_k=100)\n",
    "print(\"Baseline runs ensured:\\n\", json.dumps(baseline_runs, indent=2, default=str))\n",
    "\n",
    "# Instantiate expander (requires API_KEY in environment or .env)\n",
    "expander = TogetherQueryExpander(strategy=ExpansionStrategy.ANALYZE_GENERATE_REFINE)\n",
    "\n",
    "# Ensure method runs exist\n",
    "method_runs = ensure_runs(\n",
    "    method=method_name,\n",
    "    expander=expander,\n",
    "    datasets=datasets,\n",
    "    retrieval_methods=retrieval_methods,\n",
    "    top_k=100,\n",
    "    max_queries=auto_run_max_queries,\n",
    ")\n",
    "print(f\"{method_name} runs ensured:\\n\", json.dumps(method_runs, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4cf99",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58457413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for append method runs\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieve\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "        metric_path = data_base / \"evaluate\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        metrics = compute_metrics_from_files(str(run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(metric_path),\n",
    "            dataset=dataset,\n",
    "            method=method_name,\n",
    "            retrieval=retrieval,\n",
    "        )\n",
    "\n",
    "        all_metrics[(dataset, method_name, retrieval)] = metrics\n",
    "        print(f\"{retrieval} × {dataset}: nDCG@10={metrics['ndcg@10']:.4f}, MAP={metrics['map']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d796d9",
   "metadata": {},
   "source": [
    "## Compare with Baseline (Statistical Tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare append method with baseline (statistical test on nDCG@10)\n",
    "comparison_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        append_run_path = data_base / \"retrieve\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "        baseline_run_path = data_base / \"retrieve\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        stats = compare_runs(\n",
    "            str(baseline_run_path),\n",
    "            str(append_run_path),\n",
    "            str(qrels_path),\n",
    "            metric=\"ndcg@10\",\n",
    "            k=10,\n",
    "        )\n",
    "\n",
    "        comparison_results.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"retrieval\": retrieval,\n",
    "                \"baseline_mean\": stats[\"baseline_mean\"],\n",
    "                \"append_mean\": stats[\"system_mean\"],\n",
    "                \"difference\": stats[\"mean_difference\"],\n",
    "                \"p_value\": stats[\"p_value\"],\n",
    "                \"ci_lower\": stats.get(\"ci_lower\"),\n",
    "                \"ci_upper\": stats.get(\"ci_upper\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"{retrieval} × {dataset}: Δ={stats['mean_difference']:.4f}, p={stats['p_value']:.4f}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "pvals_path = data_base / \"evaluate\" / method_name / \"pvals.json\"\n",
    "ensure_directory(pvals_path.parent)\n",
    "with open(pvals_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"method\": method_name, \"comparisons\": comparison_results}, f, indent=2)\n",
    "print(f\"\\nP-values and CIs saved to: {pvals_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884d8b1",
   "metadata": {},
   "source": [
    "## Summary Table and Top Delta Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23640c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = data_base / \"evaluate\" / method_name / \"summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved to: {summary_path}\")\n",
    "\n",
    "# Show top delta queries for one example\n",
    "dataset_example = datasets[0]\n",
    "retrieval_example = retrieval_methods[0]\n",
    "\n",
    "qrels = load_qrels_file(str(data_base / \"ingest\" / dataset_example / \"qrels.csv\"))\n",
    "baseline_run = load_run_file(str(data_base / \"retrieve\" / \"baseline\" / f\"{dataset_example}_{retrieval_example}.csv\"))\n",
    "append_run = load_run_file(str(data_base / \"retrieve\" / method_name / f\"{dataset_example}_{retrieval_example}.csv\"))\n",
    "\n",
    "baseline_scores = compute_per_query_metric(baseline_run, qrels, metric=\"ndcg@10\", k=10)\n",
    "append_scores = compute_per_query_metric(append_run, qrels, metric=\"ndcg@10\", k=10)\n",
    "\n",
    "top_positive, top_negative = find_top_delta_queries(baseline_scores, append_scores, top_n=10)\n",
    "\n",
    "print(f\"\\nTop 5 Positive Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_positive[:5]:\n",
    "    print(f\"  {qid}: +{delta:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 5 Negative Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_negative[:5]:\n",
    "    print(f\"  {qid}: {delta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f384f",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56367418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nDCG@10 comparison (baseline vs append)\n",
    "from evaluate.compute_metrics import compute_metrics_from_files as load_metrics\n",
    "\n",
    "baseline_metrics = {}\n",
    "for dataset in datasets:\n",
    "    for retrieval in retrieval_methods:\n",
    "        qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "        baseline_run_path = data_base / \"retrieve\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "        baseline_metrics[(dataset, retrieval)] = load_metrics(str(baseline_run_path), str(qrels_path), k=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(retrieval_methods), figsize=(14, 6), squeeze=False)\n",
    "fig.suptitle(\"Append Method vs Baseline: nDCG@10\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "for idx, retrieval in enumerate(retrieval_methods):\n",
    "    ax = axes[0][idx]\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "\n",
    "    baseline_scores = [baseline_metrics[(d, retrieval)][\"ndcg@10\"] for d in datasets]\n",
    "    append_scores = [all_metrics[(d, method_name, retrieval)][\"ndcg@10\"] for d in datasets]\n",
    "\n",
    "    bars1 = ax.bar(x - width / 2, baseline_scores, width, label=\"Baseline\", alpha=0.8)\n",
    "    bars2 = ax.bar(x + width / 2, append_scores, width, label=\"Append\", alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "    ax.set_ylabel(\"nDCG@10\", fontsize=12)\n",
    "    ax.set_title(f\"{retrieval.upper()}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = data_base / \"evaluate\" / method_name / \"ndcg.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
