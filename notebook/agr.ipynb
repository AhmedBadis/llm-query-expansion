{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7e094b",
   "metadata": {},
   "source": [
    "# AGR (Analyze-Generate-Refine) Method Evaluation\n",
    "\n",
    "This notebook evaluates the AGR query expansion method (BM25 and TF-IDF) on TREC-COVID and Climate-Fever datasets, comparing against baseline.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all cells\n",
    "2. All required files will be created automatically if missing\n",
    "3. Metrics and plots will be saved to `data/evaluate/agr/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fdb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramib\\AppData\\Roaming\\Python\\Python313\\site-packages\\beir\\datasets\\data_loader.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GroqQueryExpander' from 'expand.expander' (C:\\Users\\ramib\\Desktop\\Lab 2025\\domain-specific-query-expansion-with-llms\\src\\expand\\expander.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     ensure_directory,\n\u001b[32m     32\u001b[39m     find_top_delta_queries,\n\u001b[32m     33\u001b[39m     create_summary_table,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnotebook\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure_runs\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexpand\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpander\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroqQueryExpander, ExpansionStrategy\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSetup complete! Project root:\u001b[39m\u001b[33m\"\u001b[39m, project_root)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'GroqQueryExpander' from 'expand.expander' (C:\\Users\\ramib\\Desktop\\Lab 2025\\domain-specific-query-expansion-with-llms\\src\\expand\\expander.py)"
     ]
    }
   ],
   "source": [
    "# Setup: imports and path configuration\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is located at notebook/agr.ipynb\n",
    "project_root = Path.cwd().parents[0]\n",
    "\n",
    "# Define core configuration\n",
    "datasets = [\"trec_covid\", \"climate_fever\"]\n",
    "retrieval_methods = [\"bm25\", \"tfidf\"]\n",
    "method_name = \"agr\"\n",
    "data_base = project_root / \"data\"\n",
    "\n",
    "auto_run_max_queries = None  # set to an int (e.g. 50) to limit Groq calls\n",
    "\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from evaluate import (\n",
    "    compute_metrics_from_files,\n",
    "    save_metrics_to_csv,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_per_query_metric,\n",
    "    compare_runs,\n",
    ")\n",
    "from evaluate.utils import (\n",
    "    ensure_directory,\n",
    "    find_top_delta_queries,\n",
    "    create_summary_table,\n",
    ")\n",
    "from notebook.run_api import ensure_runs\n",
    "\n",
    "from expand.expander import TogetherQueryExpander, ExpansionStrategy\n",
    "\n",
    "print(\"Setup complete! Project root:\", project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86addda6",
   "metadata": {},
   "source": [
    "## Ensure ingest outputs and method runs\n",
    "\n",
    "This will:\n",
    "- Ensure ingested artifacts exist under `data/ingest/{dataset}`\n",
    "- Ensure baseline runs exist under `data/retrieve/baseline/`\n",
    "- Expand the real dataset queries using Groq\n",
    "- Run retrieval (BM25 + TF-IDF) for the expanded queries and save to `data/retrieve/agr/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directory(data_base / \"retrieve\" / method_name)\n",
    "ensure_directory(data_base / \"evaluate\" / method_name)\n",
    "\n",
    "# Ensure baseline runs exist\n",
    "baseline_runs = ensure_runs(method=\"baseline\", datasets=datasets, retrieval_methods=retrieval_methods, top_k=100)\n",
    "print(\"Baseline runs ensured:\\n\", json.dumps(baseline_runs, indent=2, default=str))\n",
    "\n",
    "# Instantiate expander (requires API_KEY in environment or .env)\n",
    "expander = TogetherQueryExpander(strategy=ExpansionStrategy.ANALYZE_GENERATE_REFINE)\n",
    "# Ensure method runs exist\n",
    "method_runs = ensure_runs(\n",
    "    method=method_name,\n",
    "    expander=expander,\n",
    "    datasets=datasets,\n",
    "    retrieval_methods=retrieval_methods,\n",
    "    top_k=100,\n",
    "    max_queries=auto_run_max_queries,\n",
    ")\n",
    "print(f\"{method_name} runs ensured:\\n\", json.dumps(method_runs, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc2728",
   "metadata": {},
   "source": [
    "## Compute Metrics for All 4 Combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for AGR method runs\n",
    "all_metrics = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieve\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "        metric_path = data_base / \"evaluate\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        metrics = compute_metrics_from_files(str(run_path), str(qrels_path), k=10)\n",
    "        save_metrics_to_csv(\n",
    "            metrics,\n",
    "            str(metric_path),\n",
    "            dataset=dataset,\n",
    "            method=method_name,\n",
    "            retrieval=retrieval,\n",
    "        )\n",
    "\n",
    "        all_metrics[(dataset, method_name, retrieval)] = metrics\n",
    "        print(f\"{retrieval} × {dataset}: nDCG@10={metrics['ndcg@10']:.4f}, MAP={metrics['map']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04789f53",
   "metadata": {},
   "source": [
    "## Compare with Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AGR method with baseline (statistical test on nDCG@10)\n",
    "comparison_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "\n",
    "    for retrieval in retrieval_methods:\n",
    "        run_path = data_base / \"retrieve\" / method_name / f\"{dataset}_{retrieval}.csv\"\n",
    "        baseline_run_path = data_base / \"retrieve\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "\n",
    "        stats = compare_runs(\n",
    "            str(baseline_run_path),\n",
    "            str(run_path),\n",
    "            str(qrels_path),\n",
    "            metric=\"ndcg@10\",\n",
    "            k=10,\n",
    "        )\n",
    "\n",
    "        comparison_results.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"retrieval\": retrieval,\n",
    "                \"baseline_mean\": stats[\"baseline_mean\"],\n",
    "                \"agr_mean\": stats[\"system_mean\"],\n",
    "                \"difference\": stats[\"mean_difference\"],\n",
    "                \"p_value\": stats[\"p_value\"],\n",
    "                \"ci_lower\": stats.get(\"ci_lower\"),\n",
    "                \"ci_upper\": stats.get(\"ci_upper\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"{retrieval} × {dataset}: Δ={stats['mean_difference']:.4f}, p={stats['p_value']:.4f}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "pvals_path = data_base / \"evaluate\" / method_name / \"pvals.json\"\n",
    "ensure_directory(pvals_path.parent)\n",
    "with open(pvals_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"method\": method_name, \"comparisons\": comparison_results}, f, indent=2)\n",
    "print(f\"\\nP-values and CIs saved to: {pvals_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71bc61",
   "metadata": {},
   "source": [
    "## Summary and Top Delta Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = create_summary_table(all_metrics)\n",
    "summary_path = data_base / \"evaluate\" / method_name / \"summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Summary Metrics Table:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved to: {summary_path}\")\n",
    "\n",
    "# Show top delta queries for one example\n",
    "dataset_example = datasets[0]\n",
    "retrieval_example = retrieval_methods[0]\n",
    "\n",
    "qrels = load_qrels_file(str(data_base / \"ingest\" / dataset_example / \"qrels.csv\"))\n",
    "baseline_run = load_run_file(str(data_base / \"retrieve\" / \"baseline\" / f\"{dataset_example}_{retrieval_example}.csv\"))\n",
    "method_run = load_run_file(str(data_base / \"retrieve\" / method_name / f\"{dataset_example}_{retrieval_example}.csv\"))\n",
    "\n",
    "baseline_scores = compute_per_query_metric(baseline_run, qrels, metric=\"ndcg@10\", k=10)\n",
    "method_scores = compute_per_query_metric(method_run, qrels, metric=\"ndcg@10\", k=10)\n",
    "\n",
    "top_positive, top_negative = find_top_delta_queries(baseline_scores, method_scores, top_n=10)\n",
    "\n",
    "print(f\"\\nTop 5 Positive Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_positive[:5]:\n",
    "    print(f\"  {qid}: +{delta:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 5 Negative Δ (nDCG@10) - {retrieval_example} × {dataset_example}:\")\n",
    "for qid, delta in top_negative[:5]:\n",
    "    print(f\"  {qid}: {delta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b03f8",
   "metadata": {},
   "source": [
    "## Plot nDCG@10 Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b38e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot nDCG@10 comparison (baseline vs AGR)\n",
    "from evaluate.compute_metrics import compute_metrics_from_files as load_metrics\n",
    "\n",
    "baseline_metrics = {}\n",
    "for dataset in datasets:\n",
    "    for retrieval in retrieval_methods:\n",
    "        qrels_path = data_base / \"ingest\" / dataset / \"qrels.csv\"\n",
    "        baseline_run_path = data_base / \"retrieve\" / \"baseline\" / f\"{dataset}_{retrieval}.csv\"\n",
    "        baseline_metrics[(dataset, retrieval)] = load_metrics(str(baseline_run_path), str(qrels_path), k=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(retrieval_methods), figsize=(14, 6), squeeze=False)\n",
    "fig.suptitle(\"AGR Method vs Baseline: nDCG@10\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "for idx, retrieval in enumerate(retrieval_methods):\n",
    "    ax = axes[0][idx]\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "\n",
    "    baseline_scores = [baseline_metrics[(d, retrieval)][\"ndcg@10\"] for d in datasets]\n",
    "    method_scores = [all_metrics[(d, method_name, retrieval)][\"ndcg@10\"] for d in datasets]\n",
    "\n",
    "    bars1 = ax.bar(x - width / 2, baseline_scores, width, label=\"Baseline\", alpha=0.8)\n",
    "    bars2 = ax.bar(x + width / 2, method_scores, width, label=\"AGR\", alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Dataset\", fontsize=12)\n",
    "    ax.set_ylabel(\"nDCG@10\", fontsize=12)\n",
    "    ax.set_title(f\"{retrieval.upper()}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = data_base / \"evaluate\" / method_name / \"ndcg.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Plot saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
