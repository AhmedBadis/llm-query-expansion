{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Test Notebook\n",
    "\n",
    "This notebook tests all evaluation functions and workflows.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "1. Press \"Run All\" to execute all tests\n",
    "2. Verifies metrics computation, statistical tests, and robustness analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T23:13:41.532074Z",
     "iopub.status.busy": "2025-12-03T23:13:41.531878Z",
     "iopub.status.idle": "2025-12-03T23:13:45.151542Z",
     "shell.execute_reply": "2025-12-03T23:13:45.150337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is located at notebook/test.ipynb\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from eval import (\n",
    "    compute_metrics_from_files,\n",
    "    load_run_file,\n",
    "    load_qrels_file,\n",
    "    compute_all_metrics,\n",
    "    compare_runs,\n",
    "    compute_query_slices,\n",
    "    load_vocabulary\n",
    ")\n",
    "from eval.utils import (\n",
    "    ensure_directory,\n",
    "    create_dummy_run_file,\n",
    "    create_dummy_qrels_file,\n",
    "    create_dummy_vocab_file\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Create Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T23:13:45.198879Z",
     "iopub.status.busy": "2025-12-03T23:13:45.198527Z",
     "iopub.status.idle": "2025-12-03T23:13:45.222623Z",
     "shell.execute_reply": "2025-12-03T23:13:45.221465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data created!\n",
      "Run file: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\test\\test_run.csv\n",
      "Qrels file: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\test\\test_qrels.csv\n",
      "Vocab file: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\test\\test_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# Create test data directory\n",
    "test_dir = project_root / 'data' / 'test'\n",
    "ensure_directory(test_dir)\n",
    "\n",
    "# Create test files\n",
    "test_run_path = test_dir / 'test_run.csv'\n",
    "test_qrels_path = test_dir / 'test_qrels.csv'\n",
    "test_vocab_path = test_dir / 'test_vocab.txt'\n",
    "\n",
    "create_dummy_run_file(str(test_run_path), num_queries=5, num_docs_per_query=20)\n",
    "create_dummy_qrels_file(str(test_qrels_path), num_queries=5, num_relevant_per_query=3)\n",
    "create_dummy_vocab_file(str(test_vocab_path), num_tokens=1000)\n",
    "\n",
    "print(\"Test data created!\")\n",
    "print(f\"Run file: {test_run_path}\")\n",
    "print(f\"Qrels file: {test_qrels_path}\")\n",
    "print(f\"Vocab file: {test_vocab_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Load Files and Compute Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T23:13:45.228460Z",
     "iopub.status.busy": "2025-12-03T23:13:45.228156Z",
     "iopub.status.idle": "2025-12-03T23:13:45.235246Z",
     "shell.execute_reply": "2025-12-03T23:13:45.234125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 queries in run\n",
      "Loaded 5 queries in qrels\n",
      "\n",
      "Metrics:\n",
      "  ndcg@10: 1.0000\n",
      "  map: 1.0000\n",
      "  recall@100: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "Metrics computation test passed!\n"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "run = load_run_file(str(test_run_path))\n",
    "qrels = load_qrels_file(str(test_qrels_path))\n",
    "\n",
    "print(f\"Loaded {len(run)} queries in run\")\n",
    "print(f\"Loaded {len(qrels)} queries in qrels\")\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(run, qrels, k=10)\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "assert all(0 <= v <= 1 for v in metrics.values()), \"Metrics should be in [0, 1]\"\n",
    "print(\"\\nMetrics computation test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Statistical Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T23:13:45.237309Z",
     "iopub.status.busy": "2025-12-03T23:13:45.237106Z",
     "iopub.status.idle": "2025-12-03T23:13:45.281175Z",
     "shell.execute_reply": "2025-12-03T23:13:45.280116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Comparison:\n",
      "  Baseline mean: 1.0000\n",
      "  System mean:   1.0000\n",
      "  Difference:    0.0000\n",
      "  p-value:       nan\n",
      "  95% CI:        [0.0000, 0.0000]\n",
      "\n",
      "Statistical tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Create second run for comparison\n",
    "test_run2_path = test_dir / 'test_run2.csv'\n",
    "create_dummy_run_file(str(test_run2_path), num_queries=5, num_docs_per_query=20)\n",
    "\n",
    "# Compare runs\n",
    "stats = compare_runs(\n",
    "    str(test_run_path),\n",
    "    str(test_run2_path),\n",
    "    str(test_qrels_path),\n",
    "    metric='ndcg@10',\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(\"Statistical Comparison:\")\n",
    "print(f\"  Baseline mean: {stats['baseline_mean']:.4f}\")\n",
    "print(f\"  System mean:   {stats['system_mean']:.4f}\")\n",
    "print(f\"  Difference:    {stats['mean_difference']:.4f}\")\n",
    "print(f\"  p-value:       {stats['p_value']:.4f}\")\n",
    "print(f\"  95% CI:        [{stats['ci_lower']:.4f}, {stats['ci_upper']:.4f}]\")\n",
    "\n",
    "print(\"\\nStatistical tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Robustness Slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T23:13:45.283333Z",
     "iopub.status.busy": "2025-12-03T23:13:45.283139Z",
     "iopub.status.idle": "2025-12-03T23:13:45.294422Z",
     "shell.execute_reply": "2025-12-03T23:13:45.292877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Familiar queries: 0\n",
      "Unfamiliar queries: 5\n",
      "Slices saved to: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\test\\test_slices.csv\n",
      "\n",
      "Robustness slices test passed!\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary\n",
    "vocab = load_vocabulary(str(test_vocab_path), top_n=1000)\n",
    "\n",
    "# Create queries dict\n",
    "queries = {qid: qid for qid in run.keys()}\n",
    "\n",
    "# Compute slices\n",
    "slices = compute_query_slices(\n",
    "    queries,\n",
    "    str(test_run_path),\n",
    "    vocab=vocab,\n",
    "    output_file=str(test_dir / 'test_slices.csv')\n",
    ")\n",
    "\n",
    "familiar = sum(1 for s in slices.values() if s['label'] == 'familiar')\n",
    "unfamiliar = len(slices) - familiar\n",
    "\n",
    "print(f\"Familiar queries: {familiar}\")\n",
    "print(f\"Unfamiliar queries: {unfamiliar}\")\n",
    "print(f\"Slices saved to: {test_dir / 'test_slices.csv'}\")\n",
    "\n",
    "print(\"\\nRobustness slices test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: End-to-End Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T23:13:45.296945Z",
     "iopub.status.busy": "2025-12-03T23:13:45.296725Z",
     "iopub.status.idle": "2025-12-03T23:13:45.313746Z",
     "shell.execute_reply": "2025-12-03T23:13:45.312573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed and saved metrics:\n",
      "  ndcg@10: 1.0000\n",
      "  map: 1.0000\n",
      "  recall@100: 1.0000\n",
      "  mrr: 1.0000\n",
      "\n",
      "Saved to: c:\\Users\\Lenovo\\CodeProjects\\VSC\\domain-specific-query-expansion-with-llms\\data\\test\\test_metrics.csv\n",
      "\n",
      "End-to-end workflow test passed!\n",
      "\n",
      "==================================================\n",
      "All tests passed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test complete workflow: load → compute → save\n",
    "from eval.compute_metrics import compute_and_save_metrics\n",
    "\n",
    "output_metric_path = test_dir / 'test_metrics.csv'\n",
    "computed = compute_and_save_metrics(\n",
    "    str(test_run_path),\n",
    "    str(test_qrels_path),\n",
    "    str(output_metric_path),\n",
    "    dataset='test',\n",
    "    method='test',\n",
    "    retrieval='test',\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(\"Computed and saved metrics:\")\n",
    "for metric_name, value in computed.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nSaved to: {output_metric_path}\")\n",
    "\n",
    "# Verify file exists\n",
    "assert output_metric_path.exists(), \"Metrics file should exist\"\n",
    "df = pd.read_csv(output_metric_path)\n",
    "assert len(df) == 1, \"Should have one row\"\n",
    "print(\"\\nEnd-to-end workflow test passed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All tests passed!\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
