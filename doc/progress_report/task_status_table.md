# Task Status Table

| Task # | Task Description | Status |
|--------|------------------|--------|
| 1 | Add programmatic ingest API (no CLI): create `src/ingest/api.py` exposing `prepare()`, `download(dataset_name)`, and `ingest(dataset_name)` that reuse existing ingest logic and materialize outputs to `output/ingest/{dataset}` (docs.jsonl, manifest.jsonl, qrels.csv, queries.csv, vocab_top50k.txt). | Done |
| 2 | Add notebook-runner helper API: create `src/notebook/run_api.py` with `run_baseline()`, `baseline_exists()`, `ensure_baseline_runs()`, and `run_method(method_name)`; orchestrate ingestion, indexing, retrieval runners, return clear booleans, and print informative messages; keep signatures simple and documented. | Done |
| 3 | Folder / filename migration (hyphen â†’ underscore): Update all hard-coded references of hyphenated dataset names to underscored names (e.g. directory trec-covid becomes trec_covid and directory climate-fever becomes climate_fever.) | Done |
| 4 | Move ingested outputs to `output/ingest/` (code-path-only): Replace all code references that read `data/ingested/...` or hyphenated ingested paths to now point to `output/ingest/{dataset}`. Ensure helper functions and constants use underscored dataset names (e.g., `trec_covid`, `climate_fever`) and return paths like `output/ingest/trec_covid/docs.jsonl`. | Done |
| 5 | Update dataset download/ingest code paths: Change download/extraction targets from `data/dataset/trec-covid.zip` and `data/dataset/climate-fever.zip` to `data/dataset/trec_covid/trec_covid.zip` and `data/dataset/climate_fever/climate_fever.zip`; update extraction logic accordingly. | Done |
| 6 | Baseline notebook: use ingest outputs and remove DUMMY support: Modify `runner/eval/baseline.ipynb` to import `ensure_baseline_runs` and call it only when baseline files are missing; remove automatic `_DUMMY` generation logic; read ingest outputs from `output/ingest/{dataset}`. | Done |
| 7 | Refactor the 4 notebooks to remove repetition: Add shared imports at top (`from src.notebook.run_api import ensure_baseline_runs, run_method` and `from src.eval.compute_metrics import compute_and_save_metrics` or similar); replace duplicated cells with calls to `src/notebook/run_api.py` and `src/eval/*`; target notebooks: `baseline.ipynb`, `append.ipynb`, `reformulate.ipynb`, `agr.ipynb`. | Done |
| 8 | Auto-run baseline from other notebooks: In `append.ipynb`, `reformulate.ipynb`, `agr.ipynb` add `from src.notebook.run_api import ensure_baseline_runs` and call `ensure_baseline_runs()` so baseline is programmatically created if missing (no notebook execution). | Done |
| 9 | Ensure baseline notebook consumes the ingest outputs correctly: Confirm `baseline.ipynb` loads `docs.jsonl`, `manifest.jsonl` (if they make sense to be used), `qrels.csv`, `queries.csv`, `vocab_top50k.txt` from `output/ingest/{dataset}`. | Done |
| 10 | Finalize `src/eval/robustness_slices.py`: Expose `label_queries(dataset, run_df, qrels_df, vocab_path) -> pandas.DataFrame` and `save_slices(dataset, out_csv_path)` which writes `output/eval/slice/{dataset}.csv`. | Done |
| 11 | Migrate only RELEVANT plotting cells from `output/eval/plots.ipynb`: Extract plotting/aggregation cells, distribute them into the appropriate notebooks (`baseline`, `append`, `reformulate`, `agr`) where they belong (if they provide extra value or something better than the 4 notebooks can already do), then delete `output/eval/plots.ipynb` after verification. (If the 4 notebooks already do the necessary and plots.ipynb isn't needed or would complicate matters, you may simply delete it entirely without copying any content). | Done |
| 12 | Extend `src/eval/stat_tests.py`: Add `compute_paired_bootstrap_ci(runA_df, runB_df, metric='ndcg', k=10, num_samples=1000, seed=...)` that returns bootstrap confidence intervals and p-value; ensure it works with run CSV dataframes and qrels and is importable by notebooks. | Done |
| 13 | Add tests: Extend `test/test_eval.py` with unit tests for robustness slicing heuristics (synthetic data), bootstrap CI function (simple runs with known expectations), and `ensure_baseline_runs()` behavior using mocks or temporary directories; make tests runnable from `runner/eval/test.ipynb` (no CLI pytest). | Done |
| 14 | Save p-values / CIs from notebooks: Add notebook cells that call `compute_paired_bootstrap_ci(...)` and save results to `output/eval/metric/{method}/pvals_DUMMY.json` (if DUMMY) or `pvals_{method}.json` for real runs (the only real-data-using method now is baseline); use `src/paths` helpers for file paths. | Done |
| 15 | Remove DUMMY support from baseline notebook: After migration and baseline auto-run are in place, eliminate code that creates dummy baseline runs in `baseline.ipynb` (other method notebooks may retain DUMMY logic during development if needed). | Done |

